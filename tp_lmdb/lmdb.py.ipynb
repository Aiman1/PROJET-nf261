{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "import lmdb\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np #il faut enlever np\n",
    "#import matplotlib.pyplot as plt, mpld3\n",
    "from matplotlib import style\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = \"2014-01 - Citi Bike trip data.csv\"\n",
    "chunk_size = 100\n",
    "lmdb_database = lmdb.open('bike_db', map_size = 2 ** 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = [\n",
    "        \"tripduration\",\"starttime\",\"stoptime\",\"start station id\",\"start station name\",\n",
    "        \"start station latitude\",\"start station longitude\",\n",
    "        \"end station id\",\"end station name\",\n",
    "        \"end station latitude\",\"end station longitude\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "with open(path_file) as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader)\n",
    "    line_number = 1\n",
    "    with lmdb_database.begin(write = True) as txn:\n",
    "        for row in reader :\n",
    "            notBinaryObject = row[0:11]\n",
    "            txn.put(pickle.dumps(line_number), pickle.dumps(notBinaryObject))\n",
    "            line_number = line_number + 1\n",
    "            if(line_number == 100000) : \n",
    "                break\n",
    "            #break\n",
    "        print(line_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "with lmdb_database.begin() as txn:\n",
    "    cursor = txn.cursor()\n",
    "    plt.rcParams['figure.figsize'] = (16, 9)\n",
    "    plt.style.use('ggplot')\n",
    "    dataStart = [] #initialisation d'un tableau pour tester un kmeans avec les start long et lat\n",
    "    dataEnd = [] #initialisation d'un tableau pour tester un kmeans avec les end long et lat \n",
    "    \n",
    "    for key, value in cursor:\n",
    "        dataStart.append([pickle.loads(value)[5],pickle.loads(value)[6],pickle.loads(value)[9],pickle.loads(value)[10]])\n",
    "        #dataEnd.append([pickle.loads(value)[9],pickle.loads(value)[10]])\n",
    "\n",
    "    print(dataStart[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(path_file) as csv_file:\n",
    "#    reader = csv.reader(csv_file)\n",
    "#    next(reader)\n",
    "#    data = [r for r in reader]\n",
    "#    print(data[0][0:11])\n",
    "#    print(pickle.dumps(data[0][0:11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG fini\n",
    "\n",
    "#================================================================================================================\n",
    "#\n",
    "#                                                K MEANS CLUSTERING\n",
    "#\n",
    "#================================================================================================================\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k =3, tolerance = 0.0001, max_iterations = 100):\n",
    "        self.k = k\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def Euclidean_distance(self, feat_one, feat_two):\n",
    "\n",
    "        squared_distance = 0\n",
    "        #Assuming correct input to the function where the lengths of two features are the same\n",
    "        for i in range(len(feat_one)):\n",
    "                squared_distance += (feat_one[i] - feat_two[i])**2\n",
    "\n",
    "        ed = sqrt(squared_distance)\n",
    "        return ed;\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        self.centroids = {}\n",
    "        #initialize the centroids, the first 'k' elements in the dataset will be our initial centroids\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i][0:2]\n",
    "\n",
    "        #begin iterations\n",
    "        for i in range(self.max_iterations):\n",
    "            self.classes = {}\n",
    "            for i in range(self.k):\n",
    "                self.classes[i] = []\n",
    "        \n",
    "            #find the distance between the point and cluster; choose the nearest centroid\n",
    "            for features in data:\n",
    "                distances = [self.Euclidean_distance(features[0:2],self.centroids[centroid]) for centroid in self.centroids]\n",
    "                classification = distances.index(min(distances))\n",
    "                self.classes[classification].append(features)\n",
    "\n",
    "            previous = dict(self.centroids)         \n",
    "\n",
    "            #average the cluster datapoints to re-calculate the centroids\n",
    "            for classification in self.classes:\n",
    "                self.centroids[classification] = np.average(self.classes[classification][0:2], axis = 0)[0:2]\n",
    "                #il faut remplacer np\n",
    "            \n",
    "            isOptimal = True\n",
    "\n",
    "            for centroid in self.centroids:\n",
    "\n",
    "                original_centroid = previous[centroid]\n",
    "                curr = self.centroids[centroid]\n",
    "                \n",
    "                #il faut remplacer np\n",
    "                if np.sum((curr - original_centroid)/original_centroid * 100.0) > self.tolerance:\n",
    "                    isOptimal = False\n",
    "\n",
    "            #break out of the main loop if the results are optimal, ie. the centroids don't change their positions much(more than our tolerance)\n",
    "            if isOptimal:\n",
    "                break\n",
    "\n",
    "    def pred(self, data):\n",
    "        distances = [self.Euclidean_distance(data,self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = K_Means(3)\n",
    "dataNumericStart = [[float(tuple[0]),float(tuple[1]),float(tuple[2]),float(tuple[3])] for tuple in dataStart]\n",
    "km.fit(dataNumericStart)\n",
    "\n",
    "#idée : prendre ces K groupes et refaire un K-kmeans dans ces K groupes \n",
    "#sur les latitudes et longitudes d'arrivées "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting takes forever becuase of the number of points. However, it's working if you're patient enough.\n",
    "#colors = 10*[\"r\", \"g\", \"c\", \"b\", \"k\"]\n",
    "\n",
    "#for centroid in km.centroids:\n",
    "#    plt.scatter(km.centroids[centroid][0], km.centroids[centroid][1], s = 130, marker = \"x\")\n",
    "\n",
    "#    for classification in km.classes:\n",
    "#        color = colors[classification]\n",
    "#        for features in km.classes[classification]:\n",
    "#            plt.scatter(features[0], features[1], color = color,s = 30)\n",
    "            \n",
    "    #mpld3.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#récupération les coordonnées d'arrivées des 3 classes de départs\n",
    "endClass1 = [[tuple[2],tuple[3]] for tuple in km.classes[0]]\n",
    "endClass2 = [[tuple[2],tuple[3]] for tuple in km.classes[1]]\n",
    "endClass3 = [[tuple[2],tuple[3]] for tuple in km.classes[2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans sur chacune des classes \n",
    "kmEnd1 = K_Means(3)\n",
    "kmEnd2 = K_Means(3)\n",
    "kmEnd3 = K_Means(3)\n",
    "\n",
    "kmEnd1.fit(endClass1)\n",
    "kmEnd2.fit(endClass2)\n",
    "kmEnd3.fit(endClass3)\n",
    "\n",
    "#DONC on a 3 classes de départ\n",
    "#puis 3 sous-classes d'arrivées dans chaque classe de départ \n",
    "#->possibilité d'avoir un K-means en 4D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([ 40.72352603, -73.98371485]), 1: array([ 40.74054756, -73.99673669]), 2: array([ 40.71123893, -74.00709063])}\n",
      "{0: array([ 40.75840831, -74.00244649]), 1: array([ 40.72810024, -73.9965444 ]), 2: array([ 40.75423389, -73.9858293 ])}\n",
      "{0: array([ 40.74090066, -73.99462329]), 1: array([ 40.75553574, -73.99113636]), 2: array([ 40.75410206, -73.97906589])}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
